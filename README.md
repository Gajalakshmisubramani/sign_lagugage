Sign Language Detection using YOLOv5
This project is designed to detect hand gestures for sign language using the YOLOv5 object detection model. The model recognizes five sign language gestures: "Hello", "Love You", "No", "Thank You", and "Yes."

The project includes a dataset with images and annotations for training and testing. The training set consists of images with labels that the model uses to learn the gestures. After training, the model can detect hand gestures from new images or videos.

The results of the detection are saved in a folder, where each image will have bounding boxes drawn around the detected hand gestures. The project also supports real-time detection using a webcam, which allows the model to detect gestures live.

To use the project, youâ€™ll need to clone the repository and install the required dependencies. Once everything is set up, you can train the model on the dataset and run detection scripts to analyze new images. After training, the best-performing model will be used for the detection tasks.
